# ------------------------------------------------------------------------
# RLIPv2: Fast Scaling of Relational Language-Image Pre-training
# Copyright (c) Alibaba Group. All Rights Reserved.
# Licensed under the Apache License, Version 2.0 [see LICENSE for details]
# ------------------------------------------------------------------------
# Copyright (c) Hitachi, Ltd. All Rights Reserved.
# Licensed under the Apache License, Version 2.0 [see LICENSE for details]
# ------------------------------------------------------------------------
# Modified from DETR (https://github.com/facebookresearch/detr)
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
# ------------------------------------------------------------------------
"""
COCO dataset which returns image_id for evaluation.

Mostly copy-paste from https://github.com/pytorch/vision/blob/13b35ff/references/detection/coco_utils.py
"""
from pathlib import Path

import torch
import torch.utils.data
import torchvision
from pycocotools import mask as coco_mask

import datasets.transforms as T
import json
# import sys
# sys.path.append("..")

class CocoDetection(torchvision.datasets.CocoDetection):
    def __init__(self, img_folder, ann_file, transforms, return_masks):
        super(CocoDetection, self).__init__(img_folder, ann_file)
        self._transforms = transforms
        self.prepare = ConvertCocoPolysToMask(return_masks)

    def __getitem__(self, idx):
        img, target = super(CocoDetection, self).__getitem__(idx)
        image_id = self.ids[idx]
        target = {'image_id': image_id, 'annotations': target}
        img, target = self.prepare(img, target)
        if self._transforms is not None:
            img, target = self._transforms(img, target)
        return img, target



class CocoRLIPDetection(torch.utils.data.Dataset):
    def __init__(self, img_set,
                       anno_file,
                       keep_names_freq_file, 
                       transforms, 
                       num_queries, 
                       dataset = [],
                       relation_threshold = 0.,
                       use_all_text_labels = False,
                       coco2017_folder = None):
        '''
        The dataset list contains choices from ['coco2017']. 
        '''
        # for d in dataset:
        #     assert eval(f'{d}_folder') is not None
        #     eval(f'self.{d}_folder') = eval(f'{d}_folder')
        # if 'vg' in dataset:
        #     self.vg_folder = vg_folder
        if 'coco2017' in dataset:
            self.coco2017_folder = coco2017_folder
        # if 'o365' in dataset:
        #     self.o365_folder = o365_folder

        with open(anno_file, 'r') as f:
            self.annotations = json.load(f)
        self.img_set = img_set
        self._transforms = transforms
        self.num_queries = num_queries
        self.num_pairs = num_queries//2
        self.relation_threshold = relation_threshold
        self.use_all_text_labels = use_all_text_labels
        print(f'We filter every image to have no more than {self.num_pairs} triplets.')

        ### Perform thresholding
        if self.relation_threshold > 0.:
            assert self.relation_threshold > 0. and self.relation_threshold <= 1.
            new_annotations = []
            total_rel = 0
            for anno in self.annotations:
                new_annotations.append(anno)
                new_rels = []
                for rel in anno["relationships"]:
                    if "confidence" in rel.keys():  # It means that these are generated by the verb tagger.
                        if rel["confidence"] >= self.relation_threshold:
                            new_rels.append(rel)
                    else:  # It means that these are possbly from the VG dataset.
                        new_rels.append(rel)
                new_annotations[-1]["relationships"] = new_rels
                total_rel += len(new_rels)
            self.annotations = new_annotations
            print(f"We have {total_rel} relationship triplets after thresholding.")

        with open(keep_names_freq_file, "r") as f:
            vg_keep_names = json.load(f)
            self.relationship_freq = vg_keep_names["relationship_freq"]
            self.object_freq = vg_keep_names["object_freq"]
            self.relationship_names = vg_keep_names["relationship_names"]
            self.object_names = vg_keep_names["object_names"]
        
        self.ids = list(range(len(self.annotations)))
    
    def __len__(self):
        return len(self.ids)
    
    def __getitem__(self, idx):
        """
        Output:
            - target: dict of multiple items
                - boxes: Tensor[num_box, 4]. \
                    Init type: x0,y0,x1,y1. unnormalized data.
                    Final type: cx,cy,w,h. normalized data. 
                        (After using the function 'self._transforms(img, target)')
        """
        img_anno = self.annotations[self.ids[idx]]
        objects_anno = img_anno['objects'] # data type: list
        relationships_anno = img_anno['relationships'] # data type: list
        
        # if "dataset" in img_anno.keys():
        #     print(img_anno['image_id'], type(img_anno['image_id']), img_anno["dataset"])
        # else:
        #     print(img_anno['image_id'], type(img_anno['image_id']))

        if "dataset" in img_anno.keys():
            if img_anno["dataset"] == "coco2017":
                img_file_name = str(img_anno['image_id']).zfill(12) + '.jpg'
                img = Image.open(self.coco2017_folder / img_file_name).convert('RGB')
        
        w, h = img.size
        
        # make sure that #queries are more than #bboxes
        if self.img_set == 'pretrain' and len(relationships_anno) > self.num_pairs:
            relationships_anno = relationships_anno[:self.num_pairs]
        # collect coordinates and names for all bboxes
        boxes = []
        for cur_box in objects_anno:
            cur_box_cor = [cur_box['x'], cur_box['y'], cur_box['x'] + cur_box['w'], cur_box['y'] + cur_box['h']]
            boxes.append(cur_box_cor)
        # guard against no boxes via resizing
        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)

        if self.use_all_text_labels:
            obj_unique = unique_name_dict_from_list(self.object_names)
            rel_unique = unique_name_dict_from_list(self.relationship_names)
        else:
            obj_unique = unique_name_dict_from_anno(objects_anno, 'objects')
            rel_unique = unique_name_dict_from_anno(relationships_anno, 'relationships')
        obj_classes = [(idx_cur_box, obj_unique[cur_box['names']]) for idx_cur_box, cur_box in enumerate(objects_anno)]
        obj_classes = torch.tensor(obj_classes, dtype=torch.int64)

        target = {}
        target['orig_size'] = torch.as_tensor([int(h), int(w)])
        target['size'] = torch.as_tensor([int(h), int(w)])
        if self.img_set == 'pretrain':
            # HICO: clamp the box and drop those unreasonable ones
            # VG: I have checked that all boxes are 
            boxes[:, 0::2].clamp_(min=0, max=w)  # xyxy    clamp x to 0~w
            boxes[:, 1::2].clamp_(min=0, max=h)  # xyxy    clamp y to 0~h

            # This 'keep' can be removed because all w and h are assured to be greater than 0.
            # keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])
            # boxes = boxes[keep]  # may have a problem
            # obj_classes = obj_classes[keep]

            # construct target dict
            target['boxes'] = boxes
            target['labels'] = obj_classes  # like [[0, 0][1, 56][2, 0][3, 0]...]
            target['iscrowd'] = torch.tensor([0 for _ in range(boxes.shape[0])])
            target['area'] = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
            if self._transforms is not None:
                img, target = self._transforms(img, target) # target['boxes'].shape and target['labels'].shape may change

            # ******This 'keep' should be maintained because self._transform may eliminate some boxes******
            # which means that target['boxes'].shape and target['labels'].shape may change
            
            kept_box_indices = [label[0] for label in target['labels']] # enumerated indices for kept (0, 1, 2, 3, 4, ...)
            # Guard against situations with target['labels'].shape[0] = 0, which can not perform target['labels'][:, 1].
            if target['labels'].shape[0] > 0:
                target['labels'] = target['labels'][:, 1] # object classes in 80 classes

            sub_labels, obj_labels, predicate_labels, sub_boxes, obj_boxes = [], [], [], [], []
            sub_obj_pairs = []
            relationships_anno_local = add_local_object_id(relationships_anno, objects_anno)
            for cur_rel_local in relationships_anno_local:
                # Make sure that sub and obj are not eliminated by self._transform.
                if cur_rel_local['subject_id_local'] not in kept_box_indices or \
                        cur_rel_local['object_id_local'] not in kept_box_indices:
                    continue

                sub_obj_pair = (cur_rel_local['subject_id_local'], cur_rel_local['object_id_local'])
                if sub_obj_pair in sub_obj_pairs:
                    predicate_labels[sub_obj_pairs.index(sub_obj_pair)][rel_unique[cur_rel_local['predicate']]] = 1
                else:
                    sub_obj_pairs.append(sub_obj_pair)
                    # genrate labels for sub, obj and predicate
                    kept_sub_idx = kept_box_indices.index(cur_rel_local['subject_id_local'])
                    kept_obj_idx = kept_box_indices.index(cur_rel_local['object_id_local'])
                    cur_sub_label = target['labels'][kept_sub_idx]
                    cur_obj_label = target['labels'][kept_obj_idx]
                    sub_labels.append(cur_sub_label)
                    obj_labels.append(cur_obj_label)
                    predicate_label = [0 for _ in range(len(rel_unique))]
                    predicate_label[rel_unique[cur_rel_local['predicate']]] = 1
                    predicate_labels.append(predicate_label)
                    
                    # generate box coordinates for sub and obj
                    # print(f"target['boxes'].shape: {target['boxes'].shape}")
                    cur_sub_box = target['boxes'][kept_sub_idx]
                    cur_obj_box = target['boxes'][kept_obj_idx]
                    sub_boxes.append(cur_sub_box)
                    obj_boxes.append(cur_obj_box)
            
            target['image_id'] = img_anno['image_id']
            target['obj_classes'] = list(dict(obj_unique).keys())
            target['verb_classes'] = list(dict(rel_unique).keys())
            # target['obj_classes'] = generate_class_names_list(obj_unique) 
            # target['predicate_classes'] = generate_class_names_list(rel_unique) 
            if len(sub_obj_pairs) == 0:
                target['obj_labels'] = torch.zeros((0,), dtype=torch.int64)
                target['sub_labels'] = torch.zeros((0,), dtype=torch.int64)
                target['verb_labels'] = torch.zeros((0, len(rel_unique)), dtype=torch.float32)
                target['sub_boxes'] = torch.zeros((0, 4), dtype=torch.float32)
                target['obj_boxes'] = torch.zeros((0, 4), dtype=torch.float32)
            else:
                # target['obj_classes'] = list(dict(obj_unique).keys())
                # target['verb_classes'] = list(dict(rel_unique).keys())
                target['obj_labels'] = torch.stack(obj_labels)
                target['sub_labels'] = torch.stack(sub_labels)
                target['verb_labels'] = torch.as_tensor(predicate_labels, dtype=torch.float32)
                target['sub_boxes'] = torch.stack(sub_boxes)
                target['obj_boxes'] = torch.stack(obj_boxes)
        
        return img, target



class CocoRelDetection(torchvision.datasets.CocoDetection):
    def __init__(self, img_folder, ann_file, transforms, return_masks, vg_rel_texts_for_coco_images):
        super(CocoRelDetection, self).__init__(img_folder, ann_file)
        self._transforms = transforms
        self.prepare = ConvertCocoPolysToMask(return_masks)

        keep_names_file = "/Path/To/jacob/RLIP/datasets/vg_keep_names_v1_no_lias_freq.json"
        with open(keep_names_file, "r") as f:
            vg_keep_names = json.load(f)
        self.relationship_names = vg_keep_names["relationship_names"]
        self.object_names = vg_keep_names["object_names"]
        if "relationship_freq" in vg_keep_names.keys():
            self.relationship_freq = vg_keep_names["relationship_freq"]
        if "object_freq" in vg_keep_names.keys():
            self.object_freq = vg_keep_names["object_freq"]

       
        # Read relation annotations of COCO images
        # file = '/Path/To/data/coco2017/annotations/vg_rel_texts_for_coco_images_greater5.json'
        print(f"Candidate file: {vg_rel_texts_for_coco_images}.")
        with open(vg_rel_texts_for_coco_images, "r") as f:
            self.coco_img_rels = json.load(f)
        # Filter out imgs without any annotation of relation text.
        new_ids = []
        for one_id in self.ids:
            if str(one_id) in self.coco_img_rels.keys():
                new_ids.append(one_id)
        self.ids = new_ids

    def __getitem__(self, idx):
        """
        Output:
            when it is in training, we would output relation annotations like vg and hico;
            when it is in inference, we would only output object annotations to obtain pesudo relation labels.
        """
        img, target = super(CocoRelDetection, self).__getitem__(idx)
        image_id = self.ids[idx]
        target = {'image_id': image_id, 'annotations': target}
        img, target = self.prepare(img, target)
        if self._transforms is not None:
            img, target = self._transforms(img, target)

        rel_cands = self.coco_img_rels[str(image_id)]
        target['relation_candidates'] = rel_cands

        return img, target


def convert_coco_poly_to_mask(segmentations, height, width):
    masks = []
    for polygons in segmentations:
        rles = coco_mask.frPyObjects(polygons, height, width)
        mask = coco_mask.decode(rles)
        if len(mask.shape) < 3:
            mask = mask[..., None]
        mask = torch.as_tensor(mask, dtype=torch.uint8)
        mask = mask.any(dim=2)
        masks.append(mask)
    if masks:
        masks = torch.stack(masks, dim=0)
    else:
        masks = torch.zeros((0, height, width), dtype=torch.uint8)
    return masks


class ConvertCocoPolysToMask(object):
    def __init__(self, return_masks=False):
        self.return_masks = return_masks

    def __call__(self, image, target):
        w, h = image.size

        image_id = target["image_id"]
        image_id = torch.tensor([image_id])

        anno = target["annotations"]

        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]

        boxes = [obj["bbox"] for obj in anno]
        # guard against no boxes via resizing
        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)
        boxes[:, 2:] += boxes[:, :2]
        boxes[:, 0::2].clamp_(min=0, max=w)
        boxes[:, 1::2].clamp_(min=0, max=h)

        classes = [obj["category_id"] for obj in anno]
        classes = torch.tensor(classes, dtype=torch.int64)

        if self.return_masks:
            segmentations = [obj["segmentation"] for obj in anno]
            masks = convert_coco_poly_to_mask(segmentations, h, w)

        keypoints = None
        if anno and "keypoints" in anno[0]:
            keypoints = [obj["keypoints"] for obj in anno]
            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)
            num_keypoints = keypoints.shape[0]
            if num_keypoints:
                keypoints = keypoints.view(num_keypoints, -1, 3)

        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])
        boxes = boxes[keep]
        classes = classes[keep]
        if self.return_masks:
            masks = masks[keep]
        if keypoints is not None:
            keypoints = keypoints[keep]

        target = {}
        target["boxes"] = boxes
        target["labels"] = classes
        if self.return_masks:
            target["masks"] = masks
        target["image_id"] = image_id
        if keypoints is not None:
            target["keypoints"] = keypoints

        # for conversion to coco api
        area = torch.tensor([obj["area"] for obj in anno])
        iscrowd = torch.tensor([obj["iscrowd"] if "iscrowd" in obj else 0 for obj in anno])
        target["area"] = area[keep]
        target["iscrowd"] = iscrowd[keep]

        target["orig_size"] = torch.as_tensor([int(h), int(w)])
        target["size"] = torch.as_tensor([int(h), int(w)])

        return image, target


def make_coco_transforms(image_set):
    '''
    Note that Normalize() will convert 'boxes' in the target from the format of 
    xyxy to cxcywh using the function box_xyxy_to_cxcywh()!!!
    '''

    normalize = T.Compose([
        T.ToTensor(),
        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]

    if image_set == 'train':
        return T.Compose([
            T.RandomHorizontalFlip(),
            T.RandomSelect(
                T.RandomResize(scales, max_size=1333),
                T.Compose([
                    T.RandomResize([400, 500, 600]),
                    T.RandomSizeCrop(384, 600),
                    T.RandomResize(scales, max_size=1333),
                ])
            ),
            normalize,
        ])
    elif image_set == 'val' or image_set == 'tagger' or image_set == 'tagger_val':
        return T.Compose([
            T.RandomResize([800], max_size=1333),
            normalize,
        ])
    # elif image_set == 'tagger_inference':
    #     return T.Compose([
    #         normalize,
    #     ])
    else:
        assert False

    raise ValueError(f'unknown {image_set}')


def build(image_set, args):
    root = Path(args.coco_path)
    assert root.exists(), f'provided COCO path {root} does not exist'
    mode = 'instances'
    PATHS = {
        "train": (root / "train2017", root / "annotations" / f'{mode}_train2017.json'),
        "val": (root / "val2017", root / "annotations" / f'{mode}_val2017.json'),
    }

    img_folder, ann_file = PATHS[image_set]
    dataset = CocoDetection(img_folder, ann_file, transforms=make_coco_transforms(image_set), return_masks=args.masks)
    return dataset

def build_cocorel(image_set, args):
    root = Path(args.coco_path)
    assert root.exists(), f'provided COCO path {root} does not exist'
    mode = 'instances'
    PATHS = {
        "train": (root / "train2017", root / "annotations" / f'{mode}_train2017.json'),
        "tagger": (root / "train2017", root / "annotations" / f'{mode}_train2017.json'),
        "tagger_val": (root / "val2017", root / "annotations" / f'{mode}_val2017.json'),
        "val": (root / "val2017", root / "annotations" / f'{mode}_val2017.json'),
    }

    # train_json = '/Path/To/data/coco2017/annotations/instances_train2017.json'
    # val_json = '/Path/To/data/coco2017/annotations/instances_val2017.json'

    img_folder, ann_file = PATHS[image_set]
    dataset = CocoRelDetection(img_folder, 
                               ann_file,
                               transforms=make_coco_transforms(image_set),
                               return_masks=args.masks,
                               vg_rel_texts_for_coco_images=args.vg_rel_texts_for_coco_images)
    return dataset    